"""Enhanced performance regression testing framework."""

import pytest
import json
import time
import statistics
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import tempfile
from unittest.mock import Mock, patch

import oe


@dataclass
class PerformanceBaseline:
    """Performance baseline for a specific model/device combination."""
    model_id: str
    device: str
    dtype: str
    fps_mean: float
    fps_std: float
    latency_mean_ms: float
    latency_std_ms: float
    memory_usage_mb: Optional[float] = None
    timestamp: str = ""
    openvino_version: str = ""
    test_conditions: Dict[str, Any] = None


@dataclass
class PerformanceTest:
    """Configuration for a performance test."""
    model_id: str
    dtype: str = "fp16"
    warmup_runs: int = 5
    benchmark_runs: int = 20
    tolerance_percent: float = 15.0  # Allow 15% performance regression
    devices: List[str] = None
    memory_threshold_mb: Optional[float] = None


class PerformanceRegression:
    """Performance regression testing framework."""
    
    def __init__(self, baselines_file: str = "tests/performance_baselines.json"):
        self.baselines_file = Path(baselines_file)
        self.baselines: Dict[str, PerformanceBaseline] = {}
        self.load_baselines()
    
    def load_baselines(self):
        """Load performance baselines from file."""
        if self.baselines_file.exists():
            try:
                with open(self.baselines_file, 'r') as f:
                    data = json.load(f)
                
                for key, baseline_data in data.items():
                    self.baselines[key] = PerformanceBaseline(**baseline_data)
            except (json.JSONDecodeError, TypeError) as e:
                print(f"Warning: Could not load baselines: {e}")
    
    def save_baselines(self):
        """Save performance baselines to file."""
        self.baselines_file.parent.mkdir(parents=True, exist_ok=True)
        
        data = {}
        for key, baseline in self.baselines.items():
            data[key] = asdict(baseline)
        
        with open(self.baselines_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    def _get_baseline_key(self, model_id: str, device: str, dtype: str) -> str:
        """Generate unique key for baseline."""
        return f"{model_id}:{device}:{dtype}"
    
    def _measure_memory_usage(self) -> Optional[float]:
        """Measure current memory usage in MB."""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return None
    
    def run_performance_test(self, test: PerformanceTest) -> Dict[str, Any]:
        \"\"\"Run performance test and compare against baselines.\"\"\"\n        results = {\n            'model_id': test.model_id,\n            'dtype': test.dtype,\n            'test_results': {},\n            'regressions': [],\n            'improvements': [],\n            'new_baselines': []\n        }\n        \n        # Test on specified devices or detect available\n        devices = test.devices or oe.devices()\n        \n        for device in devices:\n            try:\n                print(f\"Testing {test.model_id} on {device} ({test.dtype})...\")\n                \n                # Measure memory before loading\n                memory_before = self._measure_memory_usage()\n                \n                # Load model\n                oe.load(\n                    test.model_id,\n                    device_preference=[device],\n                    dtype=test.dtype\n                )\n                \n                # Measure memory after loading\n                memory_after = self._measure_memory_usage()\n                memory_usage = None\n                if memory_before and memory_after:\n                    memory_usage = memory_after - memory_before\n                \n                # Run benchmark\n                benchmark_results = oe.benchmark(\n                    warmup_runs=test.warmup_runs,\n                    benchmark_runs=test.benchmark_runs\n                )\n                \n                # Extract performance metrics\n                fps = benchmark_results.get('fps', 0)\n                latency_ms = benchmark_results.get('mean_ms', 0)\n                actual_device = benchmark_results.get('device', device)\n                \n                # Store test results\n                device_results = {\n                    'fps': fps,\n                    'latency_ms': latency_ms,\n                    'memory_usage_mb': memory_usage,\n                    'device': actual_device,\n                    'benchmark_data': benchmark_results\n                }\n                results['test_results'][actual_device] = device_results\n                \n                # Compare against baseline\n                baseline_key = self._get_baseline_key(test.model_id, actual_device, test.dtype)\n                \n                if baseline_key in self.baselines:\n                    baseline = self.baselines[baseline_key]\n                    \n                    # Check for performance regression\n                    fps_change = ((fps - baseline.fps_mean) / baseline.fps_mean) * 100\n                    latency_change = ((latency_ms - baseline.latency_mean_ms) / baseline.latency_mean_ms) * 100\n                    \n                    if fps_change < -test.tolerance_percent:\n                        results['regressions'].append({\n                            'metric': 'fps',\n                            'device': actual_device,\n                            'current': fps,\n                            'baseline': baseline.fps_mean,\n                            'change_percent': fps_change,\n                            'severity': 'high' if fps_change < -25 else 'medium'\n                        })\n                    \n                    if latency_change > test.tolerance_percent:\n                        results['regressions'].append({\n                            'metric': 'latency',\n                            'device': actual_device,\n                            'current': latency_ms,\n                            'baseline': baseline.latency_mean_ms,\n                            'change_percent': latency_change,\n                            'severity': 'high' if latency_change > 25 else 'medium'\n                        })\n                    \n                    # Track improvements\n                    if fps_change > 5:  # 5% improvement threshold\n                        results['improvements'].append({\n                            'metric': 'fps',\n                            'device': actual_device,\n                            'current': fps,\n                            'baseline': baseline.fps_mean,\n                            'change_percent': fps_change\n                        })\n                    \n                    # Check memory usage if threshold set\n                    if test.memory_threshold_mb and memory_usage:\n                        if memory_usage > test.memory_threshold_mb:\n                            results['regressions'].append({\n                                'metric': 'memory',\n                                'device': actual_device,\n                                'current': memory_usage,\n                                'threshold': test.memory_threshold_mb,\n                                'change_percent': ((memory_usage - test.memory_threshold_mb) / test.memory_threshold_mb) * 100,\n                                'severity': 'high'\n                            })\n                \n                else:\n                    # No baseline exists - create new one\n                    import datetime\n                    import openvino as ov\n                    \n                    new_baseline = PerformanceBaseline(\n                        model_id=test.model_id,\n                        device=actual_device,\n                        dtype=test.dtype,\n                        fps_mean=fps,\n                        fps_std=0.0,  # Would need multiple runs to calculate\n                        latency_mean_ms=latency_ms,\n                        latency_std_ms=0.0,\n                        memory_usage_mb=memory_usage,\n                        timestamp=datetime.datetime.now().isoformat(),\n                        openvino_version=ov.__version__,\n                        test_conditions={'warmup_runs': test.warmup_runs, 'benchmark_runs': test.benchmark_runs}\n                    )\n                    \n                    self.baselines[baseline_key] = new_baseline\n                    results['new_baselines'].append(baseline_key)\n                \n                # Cleanup\n                oe.unload()\n                \n            except Exception as e:\n                print(f\"Error testing {test.model_id} on {device}: {e}\")\n                results['test_results'][device] = {'error': str(e)}\n        \n        return results\n    \n    def run_test_suite(self, tests: List[PerformanceTest]) -> Dict[str, Any]:\n        \"\"\"Run a complete performance test suite.\"\"\"\n        suite_results = {\n            'total_tests': len(tests),\n            'completed_tests': 0,\n            'failed_tests': 0,\n            'total_regressions': 0,\n            'total_improvements': 0,\n            'test_results': {},\n            'summary': {}\n        }\n        \n        for test in tests:\n            try:\n                test_results = self.run_performance_test(test)\n                suite_results['test_results'][test.model_id] = test_results\n                suite_results['completed_tests'] += 1\n                suite_results['total_regressions'] += len(test_results['regressions'])\n                suite_results['total_improvements'] += len(test_results['improvements'])\n                \n            except Exception as e:\n                suite_results['test_results'][test.model_id] = {'error': str(e)}\n                suite_results['failed_tests'] += 1\n        \n        # Generate summary\n        suite_results['summary'] = {\n            'success_rate': (suite_results['completed_tests'] / suite_results['total_tests']) * 100,\n            'regression_rate': suite_results['total_regressions'] / max(suite_results['completed_tests'], 1),\n            'improvement_rate': suite_results['total_improvements'] / max(suite_results['completed_tests'], 1)\n        }\n        \n        # Save updated baselines\n        self.save_baselines()\n        \n        return suite_results


class TestPerformanceRegression:\n    \"\"\"Test cases for performance regression framework.\"\"\"\n    \n    def test_baseline_management(self):\n        \"\"\"Test loading and saving baselines.\"\"\"\n        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:\n            baselines_file = f.name\n        \n        # Create regression tester\n        tester = PerformanceRegression(baselines_file)\n        \n        # Add a baseline\n        baseline = PerformanceBaseline(\n            model_id=\"test/model\",\n            device=\"CPU\",\n            dtype=\"fp16\",\n            fps_mean=25.5,\n            fps_std=1.2,\n            latency_mean_ms=39.2,\n            latency_std_ms=2.1\n        )\n        \n        key = tester._get_baseline_key(\"test/model\", \"CPU\", \"fp16\")\n        tester.baselines[key] = baseline\n        \n        # Save and reload\n        tester.save_baselines()\n        tester2 = PerformanceRegression(baselines_file)\n        \n        assert key in tester2.baselines\n        loaded_baseline = tester2.baselines[key]\n        assert loaded_baseline.model_id == \"test/model\"\n        assert loaded_baseline.fps_mean == 25.5\n        \n        # Cleanup\n        Path(baselines_file).unlink()\n    \n    @patch('oe.load')\n    @patch('oe.benchmark')\n    @patch('oe.unload')\n    @patch('oe.devices')\n    def test_performance_test_execution(self, mock_devices, mock_unload, mock_benchmark, mock_load):\n        \"\"\"Test performance test execution.\"\"\"\n        # Setup mocks\n        mock_devices.return_value = ['CPU']\n        mock_benchmark.return_value = {\n            'fps': 30.0,\n            'mean_ms': 33.3,\n            'device': 'CPU',\n            'std_ms': 1.5\n        }\n        \n        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:\n            baselines_file = f.name\n        \n        tester = PerformanceRegression(baselines_file)\n        \n        # Create test\n        test = PerformanceTest(\n            model_id=\"test/model\",\n            dtype=\"fp16\",\n            warmup_runs=3,\n            benchmark_runs=10\n        )\n        \n        # Run test\n        results = tester.run_performance_test(test)\n        \n        # Verify results\n        assert results['model_id'] == \"test/model\"\n        assert 'CPU' in results['test_results']\n        assert results['test_results']['CPU']['fps'] == 30.0\n        assert len(results['new_baselines']) == 1  # Should create new baseline\n        \n        # Verify calls\n        mock_load.assert_called_once()\n        mock_benchmark.assert_called_once_with(warmup_runs=3, benchmark_runs=10)\n        mock_unload.assert_called_once()\n        \n        # Cleanup\n        Path(baselines_file).unlink()\n    \n    def test_regression_detection(self):\n        \"\"\"Test regression detection logic.\"\"\"\n        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:\n            baselines_file = f.name\n        \n        tester = PerformanceRegression(baselines_file)\n        \n        # Add baseline\n        baseline = PerformanceBaseline(\n            model_id=\"test/model\",\n            device=\"CPU\",\n            dtype=\"fp16\",\n            fps_mean=30.0,\n            fps_std=1.0,\n            latency_mean_ms=33.3,\n            latency_std_ms=1.0\n        )\n        \n        key = tester._get_baseline_key(\"test/model\", \"CPU\", \"fp16\")\n        tester.baselines[key] = baseline\n        \n        # Mock performance test with regression\n        with patch('oe.load'), patch('oe.unload'), patch('oe.devices', return_value=['CPU']):\n            with patch('oe.benchmark') as mock_benchmark:\n                # Simulate 20% performance drop\n                mock_benchmark.return_value = {\n                    'fps': 24.0,  # 20% slower\n                    'mean_ms': 41.7,  # 25% higher latency\n                    'device': 'CPU'\n                }\n                \n                test = PerformanceTest(model_id=\"test/model\", tolerance_percent=15.0)\n                results = tester.run_performance_test(test)\n                \n                # Should detect regressions\n                assert len(results['regressions']) == 2  # FPS and latency\n                \n                fps_regression = next(r for r in results['regressions'] if r['metric'] == 'fps')\n                assert fps_regression['severity'] == 'medium'\n                assert fps_regression['change_percent'] == -20.0\n                \n                latency_regression = next(r for r in results['regressions'] if r['metric'] == 'latency')\n                assert latency_regression['severity'] == 'high'  # >25% regression\n        \n        # Cleanup\n        Path(baselines_file).unlink()\n\n\n@pytest.mark.performance\nclass TestRealModelPerformance:\n    \"\"\"Real model performance tests (requires models to be installed).\"\"\"\n    \n    @pytest.mark.slow\n    def test_text_model_performance(self):\n        \"\"\"Test text model performance regression.\"\"\"\n        # Skip if model not available\n        models = oe.models.list()\n        text_models = [m for m in models if 'dialogpt' in m['name'].lower() or 'gpt' in m['name'].lower()]\n        \n        if not text_models:\n            pytest.skip(\"No text models available for performance testing\")\n        \n        tester = PerformanceRegression()\n        \n        test = PerformanceTest(\n            model_id=\"microsoft/DialoGPT-medium\",\n            dtype=\"fp16\",\n            warmup_runs=3,\n            benchmark_runs=10,\n            tolerance_percent=20.0\n        )\n        \n        results = tester.run_performance_test(test)\n        \n        # Basic validation\n        assert results['model_id'] == \"microsoft/DialoGPT-medium\"\n        assert len(results['test_results']) > 0\n        \n        # Check for critical regressions\n        high_severity_regressions = [r for r in results['regressions'] if r['severity'] == 'high']\n        if high_severity_regressions:\n            pytest.fail(f\"High severity performance regressions detected: {high_severity_regressions}\")\n    \n    @pytest.mark.slow\n    def test_vision_model_performance(self):\n        \"\"\"Test vision model performance regression.\"\"\"\n        models = oe.models.list()\n        vision_models = [m for m in models if 'stable-diffusion' in m['name'].lower() or 'diffusion' in m['name'].lower()]\n        \n        if not vision_models:\n            pytest.skip(\"No vision models available for performance testing\")\n        \n        tester = PerformanceRegression()\n        \n        test = PerformanceTest(\n            model_id=\"runwayml/stable-diffusion-v1-5\",\n            dtype=\"fp16\",\n            warmup_runs=2,  # Fewer runs for slow models\n            benchmark_runs=5,\n            tolerance_percent=25.0,  # More tolerance for complex models\n            memory_threshold_mb=2000.0\n        )\n        \n        results = tester.run_performance_test(test)\n        \n        # Validate results\n        assert results['model_id'] == \"runwayml/stable-diffusion-v1-5\"\n        \n        # Check memory usage\n        memory_regressions = [r for r in results['regressions'] if r['metric'] == 'memory']\n        if memory_regressions:\n            pytest.fail(f\"Memory usage regressions detected: {memory_regressions}\")\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\", \"-m\", \"not slow\"])