"""Automated model compatibility validation tests."""

import pytest
import json
from pathlib import Path
from typing import Dict, List, Any, Set
from dataclasses import dataclass
import tempfile
from unittest.mock import Mock, patch

import oe


@dataclass
class CompatibilityTest:
    """Configuration for model compatibility test."""
    model_id: str
    expected_devices: List[str]
    expected_precisions: List[str] 
    model_type: str  # text, image, audio, multimodal
    min_memory_mb: float
    max_load_time_seconds: float = 60.0
    requires_tokenizer: bool = True
    known_issues: List[str] = None


class ModelCompatibilityValidator:
    """Validates model compatibility across devices and configurations."""
    
    def __init__(self, compatibility_db_file: str = "tests/model_compatibility.json"):
        self.compatibility_db_file = Path(compatibility_db_file)
        self.compatibility_db: Dict[str, CompatibilityTest] = {}
        self.load_compatibility_db()
    
    def load_compatibility_db(self):
        """Load compatibility database from file."""
        if self.compatibility_db_file.exists():
            try:
                with open(self.compatibility_db_file, 'r') as f:
                    data = json.load(f)
                
                for model_id, test_data in data.items():
                    self.compatibility_db[model_id] = CompatibilityTest(**test_data)
            except (json.JSONDecodeError, TypeError) as e:
                print(f"Warning: Could not load compatibility DB: {e}")
    
    def save_compatibility_db(self):
        """Save compatibility database to file."""
        self.compatibility_db_file.parent.mkdir(parents=True, exist_ok=True)
        
        data = {}
        for model_id, test in self.compatibility_db.items():
            data[model_id] = {
                'model_id': test.model_id,
                'expected_devices': test.expected_devices,
                'expected_precisions': test.expected_precisions,
                'model_type': test.model_type,
                'min_memory_mb': test.min_memory_mb,
                'max_load_time_seconds': test.max_load_time_seconds,
                'requires_tokenizer': test.requires_tokenizer,
                'known_issues': test.known_issues or []
            }
        
        with open(self.compatibility_db_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    def test_model_loading(self, model_id: str, device: str, dtype: str) -> Dict[str, Any]:
        """Test if model loads successfully on given device/precision."""
        import time
        import traceback
        
        result = {\n            'model_id': model_id,\n            'device': device,\n            'dtype': dtype,\n            'success': False,\n            'load_time_seconds': 0,\n            'error': None,\n            'warnings': [],\n            'model_info': None\n        }\n        \n        try:\n            start_time = time.time()\n            \n            # Attempt to load model\n            oe.load(\n                model_id,\n                device_preference=[device],\n                dtype=dtype\n            )\n            \n            load_time = time.time() - start_time\n            result['load_time_seconds'] = load_time\n            \n            # Get model info\n            model_info = oe.get_info()\n            result['model_info'] = model_info\n            result['success'] = True\n            \n            # Check if loaded on expected device\n            actual_device = model_info.get('device', 'unknown')\n            if actual_device != device:\n                result['warnings'].append(\n                    f\"Model loaded on {actual_device} instead of requested {device}\"\n                )\n            \n            # Cleanup\n            oe.unload()\n            \n        except Exception as e:\n            result['error'] = str(e)\n            result['traceback'] = traceback.format_exc()\n            \n            # Try to cleanup even if loading failed\n            try:\n                oe.unload()\n            except:\n                pass\n        \n        return result\n    \n    def test_model_inference(self, model_id: str, model_type: str) -> Dict[str, Any]:\n        \"\"\"Test basic inference functionality.\"\"\"\n        result = {\n            'model_id': model_id,\n            'inference_success': False,\n            'error': None,\n            'output_type': None,\n            'output_shape': None\n        }\n        \n        try:\n            # Generate test input based on model type\n            if model_type == 'text':\n                test_input = \"Hello, this is a test.\"\n            elif model_type == 'image':\n                test_input = \"a beautiful sunset over mountains\"\n            elif model_type == 'audio':\n                # For audio models, we'd need actual audio data\n                # For now, use text input for ASR models\n                test_input = \"test audio transcription\"\n            else:\n                test_input = \"test input\"\n            \n            # Run inference\n            output = oe.infer(test_input)\n            \n            result['inference_success'] = True\n            result['output_type'] = type(output).__name__\n            \n            # Try to get output shape/size info\n            if hasattr(output, 'shape'):\n                result['output_shape'] = str(output.shape)\n            elif isinstance(output, (list, tuple)):\n                result['output_shape'] = f\"{type(output).__name__}[{len(output)}]\"\n            elif isinstance(output, str):\n                result['output_shape'] = f\"str[{len(output)}]\"\n            \n        except Exception as e:\n            result['error'] = str(e)\n        \n        return result\n    \n    def validate_model_compatibility(self, model_id: str) -> Dict[str, Any]:\n        \"\"\"Run comprehensive compatibility validation for a model.\"\"\"\n        validation_result = {\n            'model_id': model_id,\n            'overall_compatible': True,\n            'device_compatibility': {},\n            'precision_compatibility': {},\n            'inference_test': None,\n            'issues': [],\n            'warnings': []\n        }\n        \n        # Get test configuration\n        if model_id in self.compatibility_db:\n            test_config = self.compatibility_db[model_id]\n        else:\n            # Try to infer test configuration\n            try:\n                model_info = oe.models.info(model_id)\n                test_config = CompatibilityTest(\n                    model_id=model_id,\n                    expected_devices=['CPU', 'GPU', 'NPU'],\n                    expected_precisions=['fp16', 'int8'],\n                    model_type='unknown',\n                    min_memory_mb=1000.0,\n                    requires_tokenizer=True\n                )\n                \n                # Try to infer model type from remote info\n                if model_info.get('remote'):\n                    pipeline_tag = model_info['remote_info'].get('pipeline_tag', '')\n                    if 'text' in pipeline_tag or 'generation' in pipeline_tag:\n                        test_config.model_type = 'text'\n                    elif 'image' in pipeline_tag:\n                        test_config.model_type = 'image'\n                    elif 'audio' in pipeline_tag or 'speech' in pipeline_tag:\n                        test_config.model_type = 'audio'\n                \n            except Exception as e:\n                validation_result['issues'].append(f\"Could not get model info: {e}\")\n                return validation_result\n        \n        # Test device compatibility\n        available_devices = oe.devices()\n        for device in test_config.expected_devices:\n            if device in available_devices:\n                # Test FP16 on this device\n                load_result = self.test_model_loading(model_id, device, \"fp16\")\n                validation_result['device_compatibility'][device] = load_result\n                \n                if not load_result['success']:\n                    validation_result['issues'].append(\n                        f\"Failed to load on {device}: {load_result['error']}\"\n                    )\n                    validation_result['overall_compatible'] = False\n                \n                # Check load time\n                if load_result['success'] and load_result['load_time_seconds'] > test_config.max_load_time_seconds:\n                    validation_result['warnings'].append(\n                        f\"Slow loading on {device}: {load_result['load_time_seconds']:.1f}s\"\n                    )\n            else:\n                validation_result['warnings'].append(f\"Device {device} not available\")\n        \n        # Test precision compatibility\n        primary_device = next(iter(validation_result['device_compatibility'].keys()), 'CPU')\n        for precision in test_config.expected_precisions:\n            load_result = self.test_model_loading(model_id, primary_device, precision)\n            validation_result['precision_compatibility'][precision] = load_result\n            \n            if not load_result['success']:\n                validation_result['issues'].append(\n                    f\"Failed to load with {precision} precision: {load_result['error']}\"\n                )\n                validation_result['overall_compatible'] = False\n        \n        # Test inference if model loaded successfully\n        if validation_result['device_compatibility']:\n            successful_load = next(\n                (result for result in validation_result['device_compatibility'].values() if result['success']),\n                None\n            )\n            \n            if successful_load:\n                # Load model for inference test\n                try:\n                    oe.load(model_id, dtype=\"fp16\")\n                    inference_result = self.test_model_inference(model_id, test_config.model_type)\n                    validation_result['inference_test'] = inference_result\n                    \n                    if not inference_result['inference_success']:\n                        validation_result['issues'].append(\n                            f\"Inference failed: {inference_result['error']}\"\n                        )\n                        validation_result['overall_compatible'] = False\n                    \n                    oe.unload()\n                    \n                except Exception as e:\n                    validation_result['issues'].append(f\"Could not test inference: {e}\")\n        \n        return validation_result\n    \n    def validate_model_suite(self, model_ids: List[str]) -> Dict[str, Any]:\n        \"\"\"Validate compatibility for a suite of models.\"\"\"\n        suite_results = {\n            'total_models': len(model_ids),\n            'compatible_models': 0,\n            'incompatible_models': 0,\n            'model_results': {},\n            'summary': {\n                'device_support': {},\n                'precision_support': {},\n                'common_issues': []\n            }\n        }\n        \n        device_support_count = {}\n        precision_support_count = {}\n        all_issues = []\n        \n        for model_id in model_ids:\n            try:\n                result = self.validate_model_compatibility(model_id)\n                suite_results['model_results'][model_id] = result\n                \n                if result['overall_compatible']:\n                    suite_results['compatible_models'] += 1\n                else:\n                    suite_results['incompatible_models'] += 1\n                \n                # Aggregate device support\n                for device, device_result in result['device_compatibility'].items():\n                    if device not in device_support_count:\n                        device_support_count[device] = 0\n                    if device_result['success']:\n                        device_support_count[device] += 1\n                \n                # Aggregate precision support\n                for precision, precision_result in result['precision_compatibility'].items():\n                    if precision not in precision_support_count:\n                        precision_support_count[precision] = 0\n                    if precision_result['success']:\n                        precision_support_count[precision] += 1\n                \n                # Collect issues\n                all_issues.extend(result['issues'])\n                \n            except Exception as e:\n                suite_results['model_results'][model_id] = {\n                    'overall_compatible': False,\n                    'error': str(e)\n                }\n                suite_results['incompatible_models'] += 1\n        \n        # Calculate support percentages\n        total_tested = suite_results['compatible_models'] + suite_results['incompatible_models']\n        for device, count in device_support_count.items():\n            suite_results['summary']['device_support'][device] = {\n                'supported_models': count,\n                'support_percentage': (count / total_tested) * 100 if total_tested > 0 else 0\n            }\n        \n        for precision, count in precision_support_count.items():\n            suite_results['summary']['precision_support'][precision] = {\n                'supported_models': count,\n                'support_percentage': (count / total_tested) * 100 if total_tested > 0 else 0\n            }\n        \n        # Find common issues\n        issue_counts = {}\n        for issue in all_issues:\n            issue_counts[issue] = issue_counts.get(issue, 0) + 1\n        \n        suite_results['summary']['common_issues'] = [\n            {'issue': issue, 'frequency': count}\n            for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n        ]\n        \n        return suite_results


class TestModelCompatibility:\n    \"\"\"Test cases for model compatibility validation.\"\"\"\n    \n    def test_compatibility_validator_init(self):\n        \"\"\"Test compatibility validator initialization.\"\"\"\n        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:\n            test_data = {\n                \"test/model\": {\n                    \"model_id\": \"test/model\",\n                    \"expected_devices\": [\"CPU\", \"GPU\"],\n                    \"expected_precisions\": [\"fp16\"],\n                    \"model_type\": \"text\",\n                    \"min_memory_mb\": 1000.0,\n                    \"max_load_time_seconds\": 30.0,\n                    \"requires_tokenizer\": True,\n                    \"known_issues\": []\n                }\n            }\n            json.dump(test_data, f)\n            db_file = f.name\n        \n        validator = ModelCompatibilityValidator(db_file)\n        \n        assert \"test/model\" in validator.compatibility_db\n        assert validator.compatibility_db[\"test/model\"].model_type == \"text\"\n        \n        # Cleanup\n        Path(db_file).unlink()\n    \n    @patch('oe.load')\n    @patch('oe.get_info')\n    @patch('oe.unload')\n    @patch('time.time')\n    def test_model_loading_success(self, mock_time, mock_unload, mock_get_info, mock_load):\n        \"\"\"Test successful model loading.\"\"\"\n        mock_time.side_effect = [0, 1.5]  # 1.5 second load time\n        mock_get_info.return_value = {'device': 'CPU', 'dtype': 'fp16'}\n        \n        validator = ModelCompatibilityValidator()\n        result = validator.test_model_loading(\"test/model\", \"CPU\", \"fp16\")\n        \n        assert result['success'] == True\n        assert result['load_time_seconds'] == 1.5\n        assert result['model_info']['device'] == 'CPU'\n        assert result['error'] is None\n        \n        mock_load.assert_called_once_with(\"test/model\", device_preference=[\"CPU\"], dtype=\"fp16\")\n        mock_unload.assert_called_once()\n    \n    @patch('oe.load')\n    @patch('oe.unload')\n    def test_model_loading_failure(self, mock_unload, mock_load):\n        \"\"\"Test failed model loading.\"\"\"\n        mock_load.side_effect = Exception(\"Model not found\")\n        \n        validator = ModelCompatibilityValidator()\n        result = validator.test_model_loading(\"nonexistent/model\", \"CPU\", \"fp16\")\n        \n        assert result['success'] == False\n        assert \"Model not found\" in result['error']\n        assert result['model_info'] is None\n        \n        # Should still try to cleanup\n        mock_unload.assert_called_once()\n    \n    @patch('oe.infer')\n    def test_inference_testing(self, mock_infer):\n        \"\"\"Test inference functionality testing.\"\"\"\n        mock_infer.return_value = \"Generated response\"\n        \n        validator = ModelCompatibilityValidator()\n        result = validator.test_model_inference(\"test/model\", \"text\")\n        \n        assert result['inference_success'] == True\n        assert result['output_type'] == 'str'\n        assert result['output_shape'] == 'str[18]'\n        assert result['error'] is None\n        \n        mock_infer.assert_called_once_with(\"Hello, this is a test.\")\n    \n    @patch('oe.infer')\n    def test_inference_failure(self, mock_infer):\n        \"\"\"Test inference failure handling.\"\"\"\n        mock_infer.side_effect = Exception(\"Inference failed\")\n        \n        validator = ModelCompatibilityValidator()\n        result = validator.test_model_inference(\"test/model\", \"text\")\n        \n        assert result['inference_success'] == False\n        assert \"Inference failed\" in result['error']\n    \n    @patch('oe.devices')\n    @patch.object(ModelCompatibilityValidator, 'test_model_loading')\n    @patch.object(ModelCompatibilityValidator, 'test_model_inference')\n    def test_full_compatibility_validation(self, mock_inference, mock_loading, mock_devices):\n        \"\"\"Test full compatibility validation workflow.\"\"\"\n        mock_devices.return_value = ['CPU', 'GPU']\n        \n        # Mock successful loading on both devices\n        mock_loading.side_effect = [\n            {'success': True, 'load_time_seconds': 2.0, 'error': None, 'warnings': []},  # CPU fp16\n            {'success': True, 'load_time_seconds': 1.5, 'error': None, 'warnings': []},  # GPU fp16\n            {'success': True, 'load_time_seconds': 2.2, 'error': None, 'warnings': []},  # CPU int8\n        ]\n        \n        mock_inference.return_value = {\n            'inference_success': True,\n            'error': None,\n            'output_type': 'str',\n            'output_shape': 'str[20]'\n        }\n        \n        validator = ModelCompatibilityValidator()\n        \n        # Add test configuration\n        test_config = CompatibilityTest(\n            model_id=\"test/model\",\n            expected_devices=['CPU', 'GPU'],\n            expected_precisions=['fp16', 'int8'],\n            model_type='text',\n            min_memory_mb=1000.0,\n            max_load_time_seconds=30.0\n        )\n        validator.compatibility_db[\"test/model\"] = test_config\n        \n        with patch('oe.load'), patch('oe.unload'):\n            result = validator.validate_model_compatibility(\"test/model\")\n        \n        assert result['overall_compatible'] == True\n        assert 'CPU' in result['device_compatibility']\n        assert 'GPU' in result['device_compatibility']\n        assert 'fp16' in result['precision_compatibility']\n        assert 'int8' in result['precision_compatibility']\n        assert result['inference_test']['inference_success'] == True\n        assert len(result['issues']) == 0\n\n\n@pytest.mark.compatibility\nclass TestRealModelCompatibility:\n    \"\"\"Real model compatibility tests.\"\"\"\n    \n    @pytest.mark.slow\n    def test_popular_models_compatibility(self):\n        \"\"\"Test compatibility of popular models.\"\"\"\n        popular_models = [\n            \"microsoft/DialoGPT-medium\",\n            \"runwayml/stable-diffusion-v1-5\",\n            \"openai/whisper-base\"\n        ]\n        \n        validator = ModelCompatibilityValidator()\n        \n        # Test each model if available\n        for model_id in popular_models:\n            try:\n                # Check if model is available locally or remotely\n                model_info = oe.models.info(model_id)\n                if model_info['local'] or model_info['remote']:\n                    result = validator.validate_model_compatibility(model_id)\n                    \n                    # Should at least work on CPU\n                    cpu_result = result['device_compatibility'].get('CPU')\n                    if cpu_result and not cpu_result['success']:\n                        pytest.fail(f\"Model {model_id} failed to load on CPU: {cpu_result['error']}\")\n                    \n            except Exception as e:\n                print(f\"Skipping {model_id}: {e}\")\n    \n    def test_device_specific_compatibility(self):\n        \"\"\"Test device-specific compatibility issues.\"\"\"\n        available_devices = oe.devices()\n        \n        # Test that we can at least detect devices\n        assert 'CPU' in available_devices\n        \n        # If we have other devices, they should be functional\n        for device in available_devices:\n            if device != 'CPU':\n                # Try a simple operation to verify device is functional\n                try:\n                    # This would require having a model installed\n                    pass\n                except Exception as e:\n                    pytest.fail(f\"Device {device} reported as available but not functional: {e}\")\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\", \"-m\", \"not slow\"])